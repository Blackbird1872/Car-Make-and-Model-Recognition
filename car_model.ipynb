{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from heapq import nlargest\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarClassifier(nn.Module):\n",
    "    def __init__(self, image_shape, conv_layers, num_classes):\n",
    "        super(CarClassifier, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)] +\n",
    "            [nn.Conv2d(in_channels=32 * (2**n), out_channels=64 * (2**n), kernel_size=3, padding=1) for n in range(conv_layers - 1)]\n",
    "        )\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flattened_size = (2**conv_layers) * image_shape[0] * image_shape[1]\n",
    "\n",
    "        self.fc_layers = nn.ModuleList([\n",
    "            nn.Linear(self.flattened_size, 512),\n",
    "            nn.Linear(512, num_classes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = F.relu(conv(x))\n",
    "            if i == 0 or i == 2:\n",
    "                x = self.pool(x)\n",
    "\n",
    "        x = x.reshape(-1, self.flattened_size)\n",
    "\n",
    "        x = F.relu(self.fc_layers[0](x))\n",
    "        x = self.fc_layers[1](x)\n",
    "        return x\n",
    "    \n",
    "    def get_num_conv_layers(self):\n",
    "        return len(self.convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCarClassifier(nn.Module):\n",
    "    def __init__(self, image_shape, conv_layers, num_classes, conv_kernel_size, pool_kernel_size):\n",
    "        super(CarClassifier, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        in_channels = 3\n",
    "        for n in range(conv_layers):\n",
    "            out_channels = 32 * (2 ** n)\n",
    "            self.convs.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=conv_kernel_size, padding=1))\n",
    "            in_channels = out_channels  # Update in_channels for the next layer\n",
    "\n",
    "        # Same pooling layer used for all conv layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=2)\n",
    "\n",
    "        # Adjust flattened_size calculation based on the consistent kernel size\n",
    "        self.flattened_size = self.calculate_flattened_size(image_shape, conv_layers, conv_kernel_size, pool_kernel_size)\n",
    "\n",
    "        self.fc_layers = nn.ModuleList([\n",
    "            nn.Linear(self.flattened_size, 512),\n",
    "            nn.Linear(512, num_classes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = F.relu(conv(x))\n",
    "            if i == 0 or i == 2:\n",
    "                x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "\n",
    "        x = F.relu(self.fc_layers[0](x))\n",
    "        x = self.fc_layers[1](x)\n",
    "        return x\n",
    "\n",
    "    def calculate_flattened_size(self, image_shape, conv_layers, conv_kernel_size, pool_kernel_size):\n",
    "        height, width = image_shape\n",
    "\n",
    "        for i in range(conv_layers):\n",
    "            # Conv layer with padding=1 keeps size same\n",
    "            # Apply pooling size reduction after specific layers (1st and 3rd in this case)\n",
    "            if i == 0 or i == 2:  # Pooling after 1st and 3rd layers\n",
    "                height = (height - pool_kernel_size) // 2 + 1\n",
    "                width = (width - pool_kernel_size) // 2 + 1\n",
    "\n",
    "            if height <= 0 or width <= 0:\n",
    "                raise ValueError(\"Feature map size is too small after convolution and pooling layers.\")\n",
    "\n",
    "        last_layer_channels = 32 * (2 ** (conv_layers - 1))\n",
    "        return height * width * last_layer_channels\n",
    "\n",
    "    def get_num_conv_layers(self):\n",
    "        return len(self.convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariedKernelSizeCarClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariedKernelSizeCarClassifier, self).__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=14, stride=4, padding=7)\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 480, kernel_size=3, padding=1)\n",
    "\n",
    "        # Max Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "\n",
    "        # Calculate the size of the flattened features for the fully connected layer\n",
    "        self.num_flat_features = self._get_conv_output((3, 100, 100))\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(self.num_flat_features, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 200)  # Output layer for 200 classes\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        # Helper function to calculate the size of the flattened features\n",
    "        batch_size = 1\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "        output_feat = self._forward_features(input)\n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        # Helper function to pass a dummy input through the conv layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self._forward_features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)  # Output layer for 200 classes\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDataset(Dataset):\n",
    "    def __init__(self, file_paths, image_size, csv_file):\n",
    "        self.file_paths = file_paths\n",
    "        self.image_size = image_size\n",
    "        self.idx_df = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        split = file_path.split(\"\\\\\")[1].split(\"_\")[:-1]\n",
    "\n",
    "        if len(split) > 2:\n",
    "            split = [split[0], \"_\".join(split[1:len(split)])]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(file_path).resize(self.image_size)\n",
    "            image = np.asarray(image).transpose((2, 0, 1))  # Change HWC to CHW format\n",
    "            label = self.idx_df.loc[(self.idx_df['make'] == split[0]) & (self.idx_df['model'] == split[1]), 'idx'].values[0]\n",
    "            return torch.from_numpy(image).float(), label\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"skipping image of make {split[0]} and model {split[1]}\")\n",
    "            # Return a placeholder or zero tensor\n",
    "            placeholder_image = torch.zeros((3, *self.image_size))  # Assuming 3 channels\n",
    "            placeholder_label = -1  # Assuming -1 indicates an invalid label\n",
    "            return placeholder_image, placeholder_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, image_size, training_data, validation_data, batch_size=32, epochs=20, patience=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(CarDataset(training_data, tuple(image_size), \"idx.csv\"), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for data_batch_tensor, labels in tqdm(train_loader):\n",
    "            if -1 in labels:\n",
    "                valid_mask = labels != -1\n",
    "\n",
    "                # Skip the batch if all labels are invalid\n",
    "                if not valid_mask.any():\n",
    "                    print(\"skipping batch\")\n",
    "                    continue\n",
    "\n",
    "                # Filter out invalid data and labels\n",
    "                data_batch_tensor = data_batch_tensor[valid_mask]\n",
    "                labels = labels[valid_mask]\n",
    "\n",
    "            data_batch_tensor, labels = data_batch_tensor.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch_tensor).float()\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch != 0  and epoch % 20 == 0:\n",
    "            try:\n",
    "                torch.save(model, f\"model_conv_{model.get_num_conv_layers()}_epcohs_{epoch}.pth\")\n",
    "            except:\n",
    "                torch.save(model, f\"model_epcohs_{epoch}.pth\")\n",
    "            print(f\"epoch {epoch} complete and checkpoint saved.\")\n",
    "\n",
    "    print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        total_size += sum(os.path.getsize(os.path.join(root, name)) for name in files)\n",
    "    return total_size\n",
    "\n",
    "images_path = \"VMMRdb\"\n",
    "\n",
    "if os.path.isfile(\"split.json\") and os.path.isfile(\"folder.json\"):\n",
    "    with open(\"split.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    train_files = data[\"train\"]\n",
    "    validation_files = data[\"validate\"]\n",
    "    test_files = data[\"test\"]\n",
    "\n",
    "    with open(\"folder.json\", \"r\") as f:\n",
    "        folders = json.load(f)\n",
    "\n",
    "else:\n",
    "    subdirs = [os.path.join(images_path, d) for d in os.listdir(images_path) if os.path.isdir(os.path.join(images_path, d))]\n",
    "\n",
    "    # Calculate folder sizes\n",
    "    folder_sizes = {folder: get_folder_size(folder) for folder in subdirs}\n",
    "\n",
    "    # Get the top N largest folders\n",
    "    largest = nlargest(1000, folder_sizes, key=folder_sizes.get)\n",
    "\n",
    "    # Correct way to generate filenames list\n",
    "    filenames = []\n",
    "    folders = []\n",
    "    for folder in largest:\n",
    "        folder_split = folder.split(\"_\")[:-1]\n",
    "        folder_split[0] = folder_split[0].split(\"\\\\\")[-1]\n",
    "    \n",
    "        if len(folder_split) > 2:\n",
    "            folder_split = [folder_split[0], \"_\".join(folder_split[1:len(folder_split)])]\n",
    "\n",
    "        folders.append(folder_split)\n",
    "\n",
    "        for filename in os.listdir(folder):\n",
    "            if os.path.isfile(os.path.join(folder, filename)):\n",
    "                filenames.append(os.path.join(folder, filename))\n",
    "\n",
    "    random.shuffle(filenames)\n",
    "    train_files, validation_files, test_files = np.split(filenames, [int(len(filenames)*0.8), int(len(filenames)*0.9)])\n",
    "\n",
    "    with open(\"split.json\", \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"train\": train_files.tolist(),\n",
    "                \"validate\": validation_files.tolist(),\n",
    "                \"test\": test_files.tolist()\n",
    "            },\n",
    "            f, indent=4\n",
    "        )\n",
    "\n",
    "    with open(\"folder.json\", \"w\") as f:\n",
    "        json.dump(folders, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = \"VMMRdb\"\n",
    "\n",
    "image_counts_df = pd.DataFrame(columns=[\"make\", \"model\", \"year\", \"count\"])\n",
    "images_df = pd.DataFrame(columns=[\"filename\", \"make\", \"model\", \"year\"])\n",
    "\n",
    "for folder in os.listdir(images_path):\n",
    "    folder_split = folder.split(\"_\")\n",
    "    \n",
    "    if len(folder_split) > 3:\n",
    "        folder_split = [folder_split[0], \"_\".join(folder_split[1:len(folder_split) - 1]), folder_split[len(folder_split) - 1]]\n",
    "\n",
    "    folder_path = os.path.join(images_path, folder)\n",
    "\n",
    "    count = len(os.listdir(folder_path))\n",
    "    image_counts_df.loc[len(image_counts_df)] = folder_split + [count]\n",
    "\n",
    "    data = [\n",
    "        {\n",
    "            \"filename\": os.path.join(images_path, folder, filename),\n",
    "            \"make\": folder_split[0],\n",
    "            \"model\": folder_split[1],\n",
    "            \"year\": folder_split[2]\n",
    "        }\n",
    "        for filename in os.listdir(folder_path)\n",
    "    ]\n",
    "\n",
    "    images_df = pd.concat([images_df, pd.DataFrame(data)], ignore_index=True)\n",
    "\n",
    "idx_df = pd.read_csv(\"idx.csv\")\n",
    "\n",
    "images_df = pd.merge(images_df, idx_df, on=['make', 'model'], how='left')\n",
    "\n",
    "image_counts_df.to_csv(\"image_counts.csv\")\n",
    "images_df.to_csv(\"image_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframe(df, make_model_pairs):\n",
    "    # Convert the list of lists into a set of tuples for faster lookup\n",
    "    make_model_set = set(tuple(pair) for pair in make_model_pairs)\n",
    "\n",
    "    # Define a function to check if the row's make-model pair is in the set\n",
    "    def is_in_pairs(row):\n",
    "        return (row['make'], row['model']) in make_model_set\n",
    "\n",
    "    # Apply the function to each row and filter the dataframe\n",
    "    filtered_df = df[df.apply(is_in_pairs, axis=1)]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"image_counts.csv\").drop_duplicates([\"make\", \"model\"])[[\"make\", \"model\"]]\n",
    "df = filter_dataframe(df, folders).reset_index()\n",
    "df[\"idx\"] = df.index\n",
    "\n",
    "df.to_csv(\"idx.csv\")\n",
    "\n",
    "num_classes = len(df)\n",
    "\n",
    "image_shape = [100, 100]\n",
    "\n",
    "for conv_layers in range(4,8):\n",
    "\n",
    "    model = CarClassifier(image_shape, conv_layers, num_classes).to(\"cuda\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        image_shape,\n",
    "        train_files,\n",
    "        validation_files,\n",
    "        batch_size=128,\n",
    "        epochs=100\n",
    "    )\n",
    "\n",
    "    torch.save(model, f\"model_conv{conv_layers}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "conv_layers = 4\n",
    "\n",
    "model = DynamicCarClassifier(image_shape, conv_layers, num_classes, 3, 3).to(\"cuda\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    image_shape,\n",
    "    train_files,\n",
    "    validation_files,\n",
    "    batch_size=128,\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "torch.save(model, f\"model_conv{conv_layers}_kernel3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = VariedKernelSizeCarClassifier().to(\"cuda\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    image_shape,\n",
    "    train_files,\n",
    "    validation_files,\n",
    "    batch_size=128,\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "torch.save(model, f\"model_conv{conv_layers}_vary.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def filter_dataframe(df, make_model_pairs):\n",
    "    # Convert the list of lists into a set of tuples for faster lookup\n",
    "    make_model_set = set(tuple(pair) for pair in make_model_pairs)\n",
    "\n",
    "    # Define a function to check if the row's make-model pair is in the set\n",
    "    def is_in_pairs(row):\n",
    "        return (row['make'], row['model']) in make_model_set\n",
    "\n",
    "    # Apply the function to each row and filter the dataframe\n",
    "    filtered_df = df[df.apply(is_in_pairs, axis=1)]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"image_counts.csv\").drop_duplicates([\"make\", \"model\"])[[\"make\", \"model\"]]\n",
    "df = filter_dataframe(df, folders).reset_index()\n",
    "df[\"idx\"] = df.index\n",
    "\n",
    "df.to_csv(\"idx.csv\")\n",
    "\n",
    "num_classes = len(df)\n",
    "\n",
    "image_shape = [100, 100]\n",
    "\n",
    "train_loader = DataLoader(CarDataset(test_files, tuple(image_shape), \"idx.csv\"), batch_size=128)\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    if file.endswith(\".pth\"):\n",
    "        model = torch.load(file).to(\"cuda\")\n",
    "        model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for data_batch_tensor, labels in tqdm(train_loader):\n",
    "            if -1 in labels:\n",
    "                valid_mask = labels != -1\n",
    "\n",
    "                if not valid_mask.any():\n",
    "                    print(\"skipping batch\")\n",
    "                    continue\n",
    "\n",
    "                data_batch_tensor = data_batch_tensor[valid_mask]\n",
    "                labels = labels[valid_mask]\n",
    "\n",
    "            # Move data and labels to the same device as the model\n",
    "            data_batch_tensor = data_batch_tensor.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(data_batch_tensor)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculating the metrics\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        metrics[file] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"accuracy\": accuracy\n",
    "        }\n",
    "    \n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images_with_labels(image_label_list, num_cols=2):\n",
    "    num_images = len(image_label_list)\n",
    "    rows = num_images // num_cols + int(num_images % num_cols > 0)\n",
    "    fig, axes = plt.subplots(rows, num_cols, figsize=(10, 5 * rows))\n",
    "\n",
    "    for i, (image_tensor, label) in enumerate(image_label_list):\n",
    "        image = image_tensor.numpy().transpose((1, 2, 0))\n",
    "        \n",
    "        # Normalize if the image is not in the [0, 1] range\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "\n",
    "        row, col = divmod(i, num_cols)\n",
    "        ax = axes[row, col] if num_images > num_cols else axes[col]\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, rows * num_cols):\n",
    "        row, col = divmod(j, num_cols)\n",
    "        if num_images > num_cols:\n",
    "            axes[row, col].axis('off')\n",
    "        else:\n",
    "            axes[col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def filter_dataframe(df, make_model_pairs):\n",
    "    # Convert the list of lists into a set of tuples for faster lookup\n",
    "    make_model_set = set(tuple(pair) for pair in make_model_pairs)\n",
    "\n",
    "    # Define a function to check if the row's make-model pair is in the set\n",
    "    def is_in_pairs(row):\n",
    "        return (row['make'], row['model']) in make_model_set\n",
    "\n",
    "    # Apply the function to each row and filter the dataframe\n",
    "    filtered_df = df[df.apply(is_in_pairs, axis=1)]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"image_counts.csv\").drop_duplicates([\"make\", \"model\"])[[\"make\", \"model\"]]\n",
    "df = filter_dataframe(df, folders).reset_index()\n",
    "df[\"idx\"] = df.index\n",
    "\n",
    "df.to_csv(\"idx.csv\")\n",
    "\n",
    "num_classes = len(df)\n",
    "\n",
    "image_shape = [100, 100]\n",
    "\n",
    "test_loader = DataLoader(CarDataset(test_files, tuple(image_shape), \"idx.csv\"), batch_size=128)\n",
    "\n",
    "model = torch.load(\"model_conv6.pth\").to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "found = []\n",
    "count = 0\n",
    "for data_batch_tensor, labels in test_loader:\n",
    "    if len(found) >= 4:\n",
    "        break\n",
    "\n",
    "    if -1 in labels:\n",
    "        valid_mask = labels != -1\n",
    "\n",
    "        if not valid_mask.any():\n",
    "            print(\"skipping batch\")\n",
    "            continue\n",
    "\n",
    "        data_batch_tensor = data_batch_tensor[valid_mask]\n",
    "        labels = labels[valid_mask]\n",
    "\n",
    "    # Move data and labels to the same device as the model\n",
    "    data_batch_tensor = data_batch_tensor.to(\"cuda\")\n",
    "    labels = labels.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data_batch_tensor)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    for i, (label, pred) in enumerate(zip(labels, preds)):\n",
    "        if label == pred:\n",
    "            idx_df = pd.read_csv(\"idx.csv\")\n",
    "\n",
    "            # Get the make and model for the current label\n",
    "            make, model = idx_df.loc[idx_df[\"idx\"] == label.item(), [\"make\", \"model\"]].values[0]\n",
    "\n",
    "            # Concatenate make and model into a single string\n",
    "            label_str = f\"{make} {model}\"\n",
    "\n",
    "            found.append((data_batch_tensor[i].cpu(), label_str))\n",
    "\n",
    "            if len(found) >= 4:\n",
    "                break\n",
    "\n",
    "        count += 1\n",
    "\n",
    "display_images_with_labels(found)\n",
    "print(f\"Found after {count} images.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
